Charles YooInclude an R file for each classification algorithm.Use N-Fold Cross Validation to validate your model performance.Write a short summary covering the following topics:1) An introduction to the data set. We know that these are all values that contribute to credit risk, at least.Learn more about the data here: http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)•	Simply put, this data set is of a group of applicants and their credit “scores”. “1” indicates Good Credit while “2” does not. The data set also includes the applicants’ telling attributes, such as “personal status and sex” and “credit history.” The values are all numeric. 2) Example plots that show correlation between values. Make sure you explainany significant correlations you find. This will show us you understand thedata and investigated value.•	In my findings, as evident in the charts from the KNN classification algorithm, 8 through 10 is the range of k provided the lowest error results. Take the majority vote of the K’s nearest points. So in this case, we take the nearest 8 data points nearest the applicant’s and decide whether the credit is classified as “1/Good” or “2/Bad”. With Bayes, the accuracy rate was 70%.3) Explain your approach and methodologies, including a brief explanation ofKNN, Bayes Classification, and cross validation.•	KNN is a supervised learning, which simply means that some attributes (inputs) are known for the model to classify the label. It can help you predict the classification of data points by the calculated number of nearest neighbors. •	Also a supervised learning, the Naïve Bayes algorithm is a conditional probability model whose classification depends on the independent features. Bayes is generative, meaning the classification depends on “general assumptions” given about the data to classify the answers. The independent variables will affect the outcome. •	In cross validation, we use similar data to train and test so that we can reduce the discrepancies and understand the characteristics of the model. The data has to be split into two: Training and Test. When we produce a model using the training set, we have to test the model by making predictions against the test set, which contains the answers that are already known. In other words, cross validation is a way to make sure whether you mad the correct guesses.4) Detail your results for each model. Explain the strengths and weaknesses in each model.•	I cannot explain the strength of KNN in this case. Bayes is the stronger choice here because it’s “generative” where its classification is based on the general assumptions given about the data. In the credit exercise, we’re examining each predictor (24 independent variables) and analyzing its value, we can then classify which whether the credit was good or bad. 5) Report back the best error you find for each machine learning algorithm and accuracy rating.•	I could not locate the best error and accuracy for KNN. But, for the Bayes algorithm, we had 1,000 samples, 24 predictors – the independent variables – and 2 classes (Y: “1/Good” or “2/Bad”.)1000 samples  24 predictors   2 classes: '1', '2'The following table shows that Kappa SD is at 72%.usekernel	Accuracy  	Kappa	Accuracy SD	Kappa SDFALSE	0.724     	0.376  	0.0363       	0.0681  TRUE	0.713     	0.124  	0.0211       	0.06     